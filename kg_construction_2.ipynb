{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcec05b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from google.adk.models.lite_llm import LiteLlm # For OpenAI support\n",
    "\n",
    "# Convenience libraries for working with Neo4j inside of Google ADK\n",
    "from neo4j_for_adk import graphdb, tool_success, tool_error\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2c92cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-Czs443M32fVIw5zKN1oahZFhOhZuf', created=1768861868, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_deacdd5f6f', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Yes, I'm ready! How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=13, prompt_tokens=27, total_tokens=40, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "\n",
      "OpenAI ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "MODEL_GPT_4O = \"openai/gpt-4o\"\n",
    "\n",
    "llm = LiteLlm(model=MODEL_GPT_4O)\n",
    "\n",
    "# Test LLM with a direct call\n",
    "print(llm.llm_client.completion(model=llm.model, messages=[{\"role\": \"user\", \"content\": \"Are you ready?\"}], tools=[]))\n",
    "\n",
    "print(\"\\nOpenAI ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4219ff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'query_result': [{'message': 'Neo4j is Ready!'}]}\n"
     ]
    }
   ],
   "source": [
    "# Check connection to Neo4j by sending a query\n",
    "neo4j_is_ready = graphdb.send_query(\"RETURN 'Neo4j is Ready!' as message\")\n",
    "\n",
    "print(neo4j_is_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df894ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success', 'query_result': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools import load_product_nodes\n",
    "\n",
    "load_product_nodes()\n",
    "\n",
    "# expect to find non-entity nodes with a \"Product\" label\n",
    "graphdb.send_query(\"MATCH (n) WHERE NOT n:`__Entity__` return DISTINCT labels(n) as nonEntityLabels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca84e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the approved construction plan should look something like this...\n",
    "approved_construction_plan = {\n",
    "    \"Assembly\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"label\": \"Assembly\", \n",
    "        \"unique_column_name\": \"assembly_id\", \n",
    "        \"properties\": [\"assembly_name\", \"quantity\", \"product_id\"]\n",
    "    }, \n",
    "    \"Part\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"label\": \"Part\", \n",
    "        \"unique_column_name\": \"part_id\", \n",
    "        \"properties\": [\"part_name\", \"quantity\", \"assembly_id\"]\n",
    "    }, \n",
    "    \"Product\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"products.csv\", \n",
    "        \"label\": \"Product\", \n",
    "        \"unique_column_name\": \"product_id\", \n",
    "        \"properties\": [\"product_name\", \"price\", \"description\"]\n",
    "    }, \n",
    "    \"Supplier\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"suppliers.csv\", \n",
    "        \"label\": \"Supplier\", \n",
    "        \"unique_column_name\": \"supplier_id\", \n",
    "        \"properties\": [\"name\", \"specialty\", \"city\", \"country\", \"website\", \"contact_email\"]\n",
    "    }, \n",
    "    \"Contains\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"relationship_type\": \"Contains\", \n",
    "        \"from_node_label\": \"Product\", \n",
    "        \"from_node_column\": \"product_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Is_Part_Of\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"relationship_type\": \"Is_Part_Of\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Supplied_By\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"part_supplier_mapping.csv\", \n",
    "        \"relationship_type\": \"Supplied_By\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Supplier\", \n",
    "        \"to_node_column\": \"supplier_id\", \n",
    "        \"properties\": [\"supplier_name\", \"lead_time_days\", \"unit_cost\", \"minimum_order_quantity\", \"preferred_supplier\"]\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea19982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_files = [\n",
    "    \"product_reviews/gothenburg_table_reviews.md\",\n",
    "    \"product_reviews/helsingborg_dresser_reviews.md\",\n",
    "    \"product_reviews/jonkoping_coffee_table_reviews.md\",\n",
    "    \"product_reviews/linkoping_bed_reviews.md\",\n",
    "    \"product_reviews/malmo_desk_reviews.md\",\n",
    "    \"product_reviews/norrkoping_nightstand_reviews.md\",\n",
    "    \"product_reviews/orebro_lamp_reviews.md\",\n",
    "    \"product_reviews/stockholm_chair_reviews.md\",\n",
    "    \"product_reviews/uppsala_sofa_reviews.md\",\n",
    "    \"product_reviews/vasteras_bookshelf_reviews.md\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8cbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approved entities from the `ner_agent` of Lesson 7\n",
    "approved_entities = ['Product', 'Issue', 'Feature', 'Location']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e782ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approved fact types from the `relevant_fact_agent` of Lesson 7\n",
    "approved_fact_types = {'has_issue': {'subject_label': 'Product', 'predicate_label': 'has_issue', 'object_label': 'Issue'}, 'includes_feature': {'subject_label': 'Product', 'predicate_label': 'includes_feature', 'object_label': 'Feature'}, 'used_in_location': {'subject_label': 'Product', 'predicate_label': 'used_in_location', 'object_label': 'Location'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c1fd19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j_graphrag in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: fsspec<2025.0.0,>=2024.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j_graphrag) (2024.12.0)\n",
      "Requirement already satisfied: json-repair<0.45.0,>=0.44.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j_graphrag) (0.44.1)\n",
      "Requirement already satisfied: neo4j<7.0.0,>=5.17.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j_graphrag) (6.0.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=2.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j_graphrag) (2.3.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j_graphrag) (2.12.5)\n",
      "Requirement already satisfied: pypdf<7.0.0,>=6.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j_graphrag) (6.6.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /home/codespace/.local/lib/python3.12/site-packages (from neo4j_graphrag) (6.0.3)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.13.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j_graphrag) (1.17.0)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j_graphrag) (9.1.2)\n",
      "Requirement already satisfied: types-pyyaml<7.0.0,>=6.0.12.20240917 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j_graphrag) (6.0.12.20250915)\n",
      "Requirement already satisfied: pytz in /usr/local/python/3.12.1/lib/python3.12/site-packages (from neo4j<7.0.0,>=5.17.0->neo4j_graphrag) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.3->neo4j_graphrag) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.3->neo4j_graphrag) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.3->neo4j_graphrag) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.3->neo4j_graphrag) (0.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install neo4j_graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fde4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    " \n",
    "# for example, creating a KG pipeline requires these arguments\n",
    "if False:\n",
    "    example = SimpleKGPipeline(\n",
    "        llm=None, # the LLM to use for Entity and Relation extraction\n",
    "        driver=None,  # a neo4j driver to write results to graph\n",
    "        embedder=None,  # an Embedder for chunks\n",
    "        from_pdf=True,   # sortof True because you will use a custom loader\n",
    "        pdf_loader=None, # the custom loader for Markdown\n",
    "        text_splitter=None, # the splitter you defined above\n",
    "        schema=None, # that you just defined above\n",
    "        prompt_template=None, # the template used for entity extraction on each chunk\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1023e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe10d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.experimental.components.text_splitters.base import TextSplitter\n",
    "from neo4j_graphrag.experimental.components.types import TextChunk, TextChunks\n",
    "\n",
    "# Define a custom text splitter. Chunking strategy could be yet-another-agent\n",
    "class RegexTextSplitter(TextSplitter):\n",
    "    \"\"\"Split text using regex matched delimiters.\"\"\"\n",
    "    def __init__(self, re: str):\n",
    "        self.re = re\n",
    "    \n",
    "    async def run(self, text: str) -> TextChunks:\n",
    "        \"\"\"Splits a piece of text into chunks.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be split.\n",
    "\n",
    "        Returns:\n",
    "            TextChunks: A list of chunks.\n",
    "        \"\"\"\n",
    "        texts = re.split(self.re, text)\n",
    "        i = 0\n",
    "        chunks = [TextChunk(text=str(text), index=i) for (i, text) in enumerate(texts)]\n",
    "        return TextChunks(chunks=chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f31897c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom file data loader\n",
    "\n",
    "from neo4j_graphrag.experimental.components.pdf_loader import DataLoader\n",
    "from neo4j_graphrag.experimental.components.types import PdfDocument, DocumentInfo\n",
    "\n",
    "class MarkdownDataLoader(DataLoader):\n",
    "    def extract_title(self,markdown_text):\n",
    "        # Define a regex pattern to match the first h1 header\n",
    "        pattern = r'^# (.+)$'\n",
    "\n",
    "        # Search for the first match in the markdown text\n",
    "        match = re.search(pattern, markdown_text, re.MULTILINE)\n",
    "\n",
    "        # Return the matched group if found\n",
    "        return match.group(1) if match else \"Untitled\"\n",
    "\n",
    "    async def run(self, filepath: Path, metadata = {}) -> PdfDocument:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            markdown_text = f.read()\n",
    "        doc_headline = self.extract_title(markdown_text)\n",
    "        markdown_info = DocumentInfo(\n",
    "            path=str(filepath),\n",
    "            metadata={\n",
    "                \"title\": doc_headline,\n",
    "            }\n",
    "        )\n",
    "        return PdfDocument(text=markdown_text, document_info=markdown_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e29b2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# create an OpenAI client for use by Neo4j GraphRAG\n",
    "llm_for_neo4j = OpenAILLM(model_name=\"gpt-4o\", model_params={\"temperature\": 0})\n",
    "\n",
    "# use OpenAI for creating embeddings\n",
    "embedder = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# use the same driver set up by neo4j_for_adk.py\n",
    "neo4j_driver = graphdb.get_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89f703d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema_node_types:  ['Product', 'Issue', 'Feature', 'Location']\n"
     ]
    }
   ],
   "source": [
    "# approved entities list can be used directly \n",
    "schema_node_types = approved_entities\n",
    "\n",
    "print(\"schema_node_types: \", schema_node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a388814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema_relationship_types:  ['HAS_ISSUE', 'INCLUDES_FEATURE', 'USED_IN_LOCATION']\n"
     ]
    }
   ],
   "source": [
    "# the keys from approved fact types dictionary can be used for relationship types\n",
    "schema_relationship_types = [key.upper() for key in approved_fact_types.keys()]\n",
    "\n",
    "print(\"schema_relationship_types: \", schema_relationship_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec93a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema_patterns: [['Product', 'HAS_ISSUE', 'Issue'], ['Product', 'INCLUDES_FEATURE', 'Feature'], ['Product', 'USED_IN_LOCATION', 'Location']]\n"
     ]
    }
   ],
   "source": [
    "# rewrite the fact types into a list of tuples\n",
    "schema_patterns = [\n",
    "    [ fact['subject_label'], fact['predicate_label'].upper(), fact['object_label'] ]\n",
    "    for fact in approved_fact_types.values()\n",
    "]\n",
    "\n",
    "print(\"schema_patterns:\", schema_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8630ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the complete entity schema\n",
    "entity_schema = {\n",
    "    \"node_types\": schema_node_types,\n",
    "    \"relationship_types\": schema_relationship_types,\n",
    "    \"patterns\": schema_patterns,\n",
    "    \"additional_node_types\": False, # True would be less strict, allowing unknown node types\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8800e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_context(file_path:str, num_lines=5) -> str:\n",
    "    \"\"\"Helper function to extract the first few lines of a file\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "        num_lines (int, optional): Number of lines to extract. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        str: First few lines of the file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = []\n",
    "        for _ in range(num_lines):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc3a316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-chunk entity extraction prompt, with context\n",
    "def contextualize_er_extraction_prompt(context:str) -> str:\n",
    "    \"\"\"Creates a prompt with pre-amble file content for context during entity+relationship extraction.\n",
    "    The context is concatenated into the string, which later will be used as a template\n",
    "    for values like {schema} and {text}.\n",
    "    \"\"\"\n",
    "    general_instructions = \"\"\"\n",
    "    You are a top-tier algorithm designed for extracting\n",
    "    information in structured formats to build a knowledge graph.\n",
    "\n",
    "    Extract the entities (nodes) and specify their type from the following text.\n",
    "    Also extract the relationships between these nodes.\n",
    "\n",
    "    Return result as JSON using the following format:\n",
    "    {{\"nodes\": [ {{\"id\": \"0\", \"label\": \"Person\", \"properties\": {{\"name\": \"John\"}} }}],\n",
    "    \"relationships\": [{{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {{\"since\": \"2024-08-01\"}} }}] }}\n",
    "\n",
    "    Use only the following node and relationship types (if provided):\n",
    "    {schema}\n",
    "\n",
    "    Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
    "    Do respect the source and target node types for relationship and\n",
    "    the relationship direction.\n",
    "\n",
    "    Make sure you adhere to the following rules to produce valid JSON objects:\n",
    "    - Do not return any additional information other than the JSON in it.\n",
    "    - Omit any backticks around the JSON - simply output the JSON on its own.\n",
    "    - The JSON object must not wrapped into a list - it is its own JSON object.\n",
    "    - Property names must be enclosed in double quotes\n",
    "    \"\"\"\n",
    "\n",
    "    context_goes_here = f\"\"\"\n",
    "    Consider the following context to help identify entities and relationships:\n",
    "    <context>\n",
    "    {context}  \n",
    "    </context>\"\"\"\n",
    "    \n",
    "    input_goes_here = \"\"\"\n",
    "    Input text:\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    return general_instructions + \"\\n\" + context_goes_here + \"\\n\" + input_goes_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16afc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kg_builder(file_path:str) -> SimpleKGPipeline:\n",
    "    \"\"\"Builds a KG builder for a given file, which is used to contextualize the chunking and entity extraction.\"\"\"\n",
    "    context = file_context(file_path)\n",
    "    contextualized_prompt = contextualize_er_extraction_prompt(context)\n",
    "\n",
    "    return SimpleKGPipeline(\n",
    "        llm=llm_for_neo4j, # the LLM to use for Entity and Relation extraction\n",
    "        driver=neo4j_driver,  # a neo4j driver to write results to graph\n",
    "        embedder=embedder,  # an Embedder for chunks\n",
    "        from_pdf=True,   # sortof True because you will use a custom loader\n",
    "        pdf_loader=MarkdownDataLoader(), # the custom loader for Markdown\n",
    "        text_splitter=RegexTextSplitter(\"---\"), # the splitter you defined above\n",
    "        schema=entity_schema, # that you just defined above\n",
    "        prompt_template=contextualized_prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a37236e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: product_reviews/gothenburg_table_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "Processing file: product_reviews/helsingborg_dresser_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "Processing file: product_reviews/jonkoping_coffee_table_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "Processing file: product_reviews/linkoping_bed_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "Processing file: product_reviews/malmo_desk_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "Processing file: product_reviews/norrkoping_nightstand_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "Processing file: product_reviews/orebro_lamp_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "Processing file: product_reviews/stockholm_chair_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "Processing file: product_reviews/uppsala_sofa_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "Processing file: product_reviews/vasteras_bookshelf_reviews.md\n",
      "\tResults: {'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': None}}\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "from helper import get_neo4j_import_dir\n",
    "\n",
    "neo4j_import_dir = get_neo4j_import_dir() or \".\"\n",
    "\n",
    "for file_name in approved_files:\n",
    "    file_path = os.path.join(neo4j_import_dir, file_name)\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    kg_builder = make_kg_builder(file_path)\n",
    "    results = await kg_builder.run_async(file_path=str(file_path))\n",
    "    print(\"\\tResults:\", results.result)\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dabce405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, take a look at the entity labels\n",
    "results = graphdb.send_query(\"\"\"MATCH (n)\n",
    "    WHERE n:`__Entity__`\n",
    "    RETURN DISTINCT labels(n) AS entity_labels\n",
    "    \"\"\")\n",
    "\n",
    "results['query_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4601292b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unwind those lists of labels\n",
    "results = graphdb.send_query(\"\"\"MATCH (n)\n",
    "    WHERE n:`__Entity__`\n",
    "    WITH DISTINCT labels(n) AS entity_labels\n",
    "    UNWIND entity_labels AS entity_label\n",
    "    RETURN DISTINCT entity_label\n",
    "    \"\"\")\n",
    "\n",
    "results['query_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6e65df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out labels that start with \"__\"\n",
    "results = graphdb.send_query(\"\"\"MATCH (n)\n",
    "    WHERE n:`__Entity__`\n",
    "    WITH DISTINCT labels(n) AS entity_labels\n",
    "    UNWIND entity_labels AS entity_label\n",
    "    WITH entity_label\n",
    "    WHERE NOT entity_label STARTS WITH \"__\"\n",
    "    RETURN entity_label\n",
    "    \"\"\")\n",
    "\n",
    "results['query_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77431be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the query into a callable function\n",
    "def find_unique_entity_labels():\n",
    "    result = graphdb.send_query(\"\"\"MATCH (n)\n",
    "        WHERE n:`__Entity__`\n",
    "        WITH DISTINCT labels(n) AS entity_labels\n",
    "        UNWIND entity_labels AS entity_label\n",
    "        WITH entity_label\n",
    "        WHERE NOT entity_label STARTS WITH \"__\"\n",
    "        RETURN collect(entity_label) as unique_entity_labels\n",
    "        \"\"\")\n",
    "    if result['status'] == 'error':\n",
    "        raise Exception(result['message'])\n",
    "    return result['query_result'][0]['unique_entity_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84e485b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique entity labels:  []\n"
     ]
    }
   ],
   "source": [
    "# try out the function\n",
    "unique_entity_labels = find_unique_entity_labels()\n",
    "\n",
    "print(\"Unique entity labels: \", unique_entity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d47154b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_unique_entity_keys(entityLabel:str):\n",
    "    result = graphdb.send_query(\"\"\"MATCH (n:$($entityLabel))\n",
    "    WHERE n:`__Entity__`\n",
    "    WITH DISTINCT keys(n) as entityKeys\n",
    "    UNWIND entityKeys as entityKey\n",
    "    RETURN collect(distinct(entityKey)) as unique_entity_keys\n",
    "    \"\"\", {\n",
    "        \"entityLabel\": entityLabel\n",
    "    })\n",
    "    if result['status'] == 'error':\n",
    "        raise Exception(result['message'])\n",
    "    return result['query_result'][0]['unique_entity_keys']\n",
    "        \n",
    "# try out the function to get the unique keys for \n",
    "# subject nodes labeled as Product\n",
    "find_unique_entity_keys(\"Product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "980cd1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_unique_domain_keys(domainLabel:str):\n",
    "    result = graphdb.send_query(\"\"\"MATCH (n:$($domainLabel))\n",
    "    WHERE NOT n:`__Entity__` // exclude entities created by the KG builder, these should be domain nodes\n",
    "    WITH DISTINCT keys(n) as domainKeys\n",
    "    UNWIND domainKeys as domainKey\n",
    "    RETURN collect(distinct(domainKey)) as unique_domain_keys\n",
    "    \"\"\", {\n",
    "        \"domainLabel\": domainLabel\n",
    "    })\n",
    "    if result['status'] == 'error':\n",
    "        raise Exception(result['message'])\n",
    "    return result['query_result'][0]['unique_domain_keys']\n",
    "        \n",
    "\n",
    "find_unique_domain_keys(\"Product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecf0e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "name\n",
      "name\n",
      "price\n"
     ]
    }
   ],
   "source": [
    "def normalize_key(label:str, key:str) -> str:\n",
    "    \"\"\"Normalizes a a property key for a given label.\n",
    "\n",
    "    Keys are normalized by:\n",
    "    - lowercase the key\n",
    "    - remove any leading/trailing whitespace\n",
    "    - remove label prefix from key\n",
    "    - replace internal whitespace with \"_\"\n",
    "\n",
    "    for example: \n",
    "        - \"Product_name\" -> \"name\"\n",
    "        - \"product name\" -> \"name\"\n",
    "        - \"price\" -> \"price\n",
    "\n",
    "    Args:\n",
    "        label (str): The label to normalize keys for\n",
    "        keys (List[str]): The list of keys to normalize\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The normalized list of keys\n",
    "    \"\"\"\n",
    "    lowercase_key = key.lower()\n",
    "    unprefixed_key = re.sub(f\"^{label.lower()}[_ ]*\", \"\", lowercase_key)\n",
    "    normalized_key = re.sub(\" \", \"_\", unprefixed_key)\n",
    "    return normalized_key\n",
    "\n",
    "print(normalize_key(\"Product\", \"Product_name\"))\n",
    "print(normalize_key(\"Product\", \"Product Name\"))\n",
    "print(normalize_key(\"Product\", \"product name\"))\n",
    "print(normalize_key(\"Product\", \"price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78530a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.14.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b84d605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product correlated keys (entity key, domain key, similarity score)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the rapidfuzz library for fuzzy text similarity scoring\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# for a given label, get pairs of entity and domain keys that correlate\n",
    "def correlate_entity_and_domain_keys(label: str, entity_keys: list[str], domain_keys: list[str], similarity: float = 0.9) -> list[tuple[str, str]]:\n",
    "    correlated_keys = []\n",
    "    for entity_key in entity_keys:\n",
    "        for domain_key in domain_keys:\n",
    "            # only consider exact matches. this could use fuzzy matching\n",
    "            normalized_entity_key = normalize_key(label, entity_key)\n",
    "            normalized_domain_key = normalize_key(label, domain_key)\n",
    "            # rapidfuzz similarity is 0.0 -> 100.0, so divide by 100 for 0.0 -> 1.0\n",
    "            fuzzy_similarity = (fuzz.ratio(normalized_entity_key, normalized_domain_key) / 100)\n",
    "            if (fuzzy_similarity > similarity): \n",
    "                correlated_keys.append((entity_key, domain_key, fuzzy_similarity))\n",
    "    correlated_keys.sort(key=lambda x: x[2], reverse=True)\n",
    "    return correlated_keys\n",
    "\n",
    "label = \"Product\"\n",
    "entity_keys = find_unique_entity_keys(label)\n",
    "domain_keys = find_unique_domain_keys(label)\n",
    "\n",
    "# try correlating with a low-ish threshold\n",
    "correlated_keys = correlate_entity_and_domain_keys(label, entity_keys, domain_keys, similarity=0.5)\n",
    "\n",
    "print(f\"{label} correlated keys (entity key, domain key, similarity score)...\")\n",
    "\n",
    "# show the keys\n",
    "correlated_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2879fdf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'query_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# use the Jaro-Winkler function to calculate distance between product names\u001b[39;00m\n\u001b[32m      2\u001b[39m results = graphdb.send_query(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m// MATCH all pairs of subject and domain nodes -- this is an expensive cartesian product\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mMATCH (entity:$($entityLabel):`__Entity__`), (domain:$($entityLabel))\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdomainKey\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mproduct_name\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquery_result\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'query_result'"
     ]
    }
   ],
   "source": [
    "# use the Jaro-Winkler function to calculate distance between product names\n",
    "results = graphdb.send_query(\"\"\"\n",
    "// MATCH all pairs of subject and domain nodes -- this is an expensive cartesian product\n",
    "MATCH (entity:$($entityLabel):`__Entity__`), (domain:$($entityLabel))\n",
    "WITH entity, domain, apoc.text.jaroWinklerDistance(entity[$entityKey], domain[$domainKey]) as score\n",
    "// experiment with different thresholds to see how the results change\n",
    "WHERE score < 0.4\n",
    "RETURN entity[$entityKey] AS entityValue, domain[$domainKey] AS domainValue, score\n",
    "// experiment with different limits to see more or fewer pairs\n",
    "LIMIT 3\n",
    "\"\"\", {\n",
    "    \"entityLabel\": \"Product\",\n",
    "    \"entityKey\": \"name\",\n",
    "    \"domainKey\": \"product_name\"\n",
    "})\n",
    "\n",
    "results['query_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0626a731",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'query_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# connect all corresponding nodes with a relationship\u001b[39;00m\n\u001b[32m      2\u001b[39m results = graphdb.send_query(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mMATCH (entity:$($entityLabel):`__Entity__`),(domain:$($entityLabel))\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m// use the score as a predicate to filter the pairs. this is better\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdomainKey\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mproduct_name\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquery_result\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# run this repeatedly to illustrate that MERGE only happens once\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'query_result'"
     ]
    }
   ],
   "source": [
    "# connect all corresponding nodes with a relationship\n",
    "results = graphdb.send_query(\"\"\"\n",
    "MATCH (entity:$($entityLabel):`__Entity__`),(domain:$($entityLabel))\n",
    "// use the score as a predicate to filter the pairs. this is better\n",
    "WHERE apoc.text.jaroWinklerDistance(entity[$entityKey], domain[$domainKey]) < 0.1\n",
    "MERGE (entity)-[r:CORRESPONDS_TO]->(domain)\n",
    "ON CREATE SET r.created_at = datetime()\n",
    "ON MATCH SET r.updated_at = datetime()\n",
    "RETURN elementId(entity) as entity_id, r, elementId(domain) as domain_id\n",
    "\"\"\", {\n",
    "    \"entityLabel\": \"Product\",\n",
    "    \"entityKey\": \"name\",\n",
    "    \"domainKey\": \"product_name\"\n",
    "})\n",
    "\n",
    "results['query_result']\n",
    "\n",
    "# run this repeatedly to illustrate that MERGE only happens once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "733ecce4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(results[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[33m'\u001b[39m\u001b[33mquery_result\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mcorrelate_subject_and_domain_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProduct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproduct_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mcorrelate_subject_and_domain_nodes\u001b[39m\u001b[34m(label, entity_key, domain_key, similarity)\u001b[39m\n\u001b[32m     20\u001b[39m results = graphdb.send_query(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[33mMATCH (entity:$($entityLabel):`__Entity__`),(domain:$($entityLabel))\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[33mWHERE apoc.text.jaroWinklerDistance(entity[$entityKey], domain[$domainKey]) < $distance\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdistance\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m1.0\u001b[39m - similarity)\n\u001b[32m     32\u001b[39m })\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[33m'\u001b[39m\u001b[33mquery_result\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'message'"
     ]
    }
   ],
   "source": [
    "# wrap as a function\n",
    "def correlate_subject_and_domain_nodes(label: str, entity_key: str, domain_key: str, similarity: float = 0.9) -> dict:\n",
    "    \"\"\"Correlate entity and domain nodes based on label, entity key, and domain key,\n",
    "    where the corresponding values of the entity and domain properties are similar\n",
    "    \n",
    "    For example, if you have a label \"Person\" and an entity key \"name\", and a domain key \"person_name\",\n",
    "    this function will create a relationship like:\n",
    "    (:Person:`__Entity__` {name: \"John\"})-[:CORRELATES_TO]->(:Person {person_name: \"John\"}) \n",
    "    \n",
    "\n",
    "    Args:\n",
    "        label (str): The label of the entity and domain nodes.\n",
    "        entity_key (str): The key of the entity node.\n",
    "        domain_key (str): The key of the domain node.\n",
    "        similarity (float, optional): The similarity threshold for correlation. Defaults to 0.9.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the correlation between the entity and domain nodes.\n",
    "    \"\"\"\n",
    "    results = graphdb.send_query(\"\"\"\n",
    "    MATCH (entity:$($entityLabel):`__Entity__`),(domain:$($entityLabel))\n",
    "    WHERE apoc.text.jaroWinklerDistance(entity[$entityKey], domain[$domainKey]) < $distance\n",
    "    MERGE (entity)-[r:CORRESPONDS_TO]->(domain)\n",
    "    ON CREATE SET r.created_at = datetime() // MERGE sub-clause when the relationship is newly created\n",
    "    ON MATCH SET r.updated_at = datetime()  // MERGE sub-clause when the relationship already exists\n",
    "    RETURN $entityLabel as entityLabel, count(r) as relationshipCount\n",
    "    \"\"\", {\n",
    "        \"entityLabel\": label,\n",
    "        \"entityKey\": entity_key,\n",
    "        \"domainKey\": domain_key,\n",
    "        \"distance\": (1.0 - similarity)\n",
    "    })\n",
    "\n",
    "    if results['status'] == 'error':\n",
    "        raise Exception(results['message'])\n",
    "\n",
    "    return results['query_result']\n",
    "\n",
    "\n",
    "correlate_subject_and_domain_nodes(\"Product\", \"name\", \"product_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c01e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it all:\n",
    "# - loop over all entity labels\n",
    "# - correlate the keys\n",
    "# - correlate (and connect) the nodes\n",
    "for entity_label in find_unique_entity_labels():\n",
    "    print(f\"Correlating entities labeled {entity_label}...\")\n",
    "    \n",
    "    entity_keys = find_unique_entity_keys(entity_label)\n",
    "    domain_keys = find_unique_domain_keys(entity_label)\n",
    "\n",
    "    correlated_keys = correlate_entity_and_domain_keys(entity_label, entity_keys, domain_keys, similarity=0.8)\n",
    "\n",
    "    if (len(correlated_keys) > 0):\n",
    "        top_correlated_keypair = correlated_keys[0]\n",
    "        print(\"\\tbased on:\", top_correlated_keypair)\n",
    "        correlate_subject_and_domain_nodes(entity_label, top_correlated_keypair[0], top_correlated_keypair[1])\n",
    "    else:\n",
    "        print(\"\\tNo correlation found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
